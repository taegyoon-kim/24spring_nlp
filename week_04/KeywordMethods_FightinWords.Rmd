---
title: "HSS 510 Week 4 Tutorial: Fightin' Words"
date: 2024-03-20
author: "Taegyoon Kim"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```
This is to give you some tools for calculating the Fightin Words statistic, extracting top-ranked terms, and plotting. This tutorial is adapted from Burt Monroe's  (the original author of the paper/method).
```

## Load Libraries (install them using `install.packages()` if you haven't)

```{r}
library(quanteda)
library(tidyr)
library(dplyr)
library(knitr)
library(ggplot2)
library(ggrepel)
library(stm)
```

## Load the FW Functions

```{r}

### fightin' words estimation

fwgroups <- function(dtm, groups, pair = NULL, weights = rep(1,nrow(dtm)), k.prior = 0.1) {
  
  weights[is.na(weights)] <- 0
  
  weights <- weights/mean(weights)
  
  zero.doc <- rowSums(dtm)==0 | weights==0
  zero.term <- colSums(dtm[!zero.doc,])==0
  
  dtm.nz <- apply(dtm[!zero.doc,!zero.term],2,"*", weights[!zero.doc])
  
  g.prior <- tcrossprod(rowSums(dtm.nz),colSums(dtm.nz))/sum(dtm.nz)
  
  # 
  
  g.posterior <- as.matrix(dtm.nz + k.prior*g.prior)
  
  groups <- groups[!zero.doc]
  groups <- droplevels(groups)
  
  g.adtm <- as.matrix(aggregate(x=g.posterior,by=list(groups=groups),FUN=sum)[,-1])
  rownames(g.adtm) <- levels(groups)
  
  g.ladtm <- log(g.adtm)
  
  g.delta <- t(scale( t(scale(g.ladtm, center=T, scale=F)), center=T, scale=F))
  
  g.adtm_w <- -sweep(g.adtm,1,rowSums(g.adtm)) # terms not w spoken by k
  g.adtm_k <- -sweep(g.adtm,2,colSums(g.adtm)) # w spoken by groups other than k
  g.adtm_kw <- sum(g.adtm) - g.adtm_w - g.adtm_k - g.adtm # total terms not w or k 
  
  g.se <- sqrt(1/g.adtm + 1/g.adtm_w + 1/g.adtm_k + 1/g.adtm_kw)
  
  g.zeta <- g.delta/g.se
  
  g.counts <- as.matrix(aggregate(x=dtm.nz, by = list(groups=groups), FUN=sum)[,-1])
  
  if (!is.null(pair)) {
    pr.delta <- t(scale( t(scale(g.ladtm[pair,], center = T, scale =F)), center=T, scale=F))
    pr.adtm_w <- -sweep(g.adtm[pair,],1,rowSums(g.adtm[pair,]))
    pr.adtm_k <- -sweep(g.adtm[pair,],2,colSums(g.adtm[pair,])) # w spoken by groups other than k
    pr.adtm_kw <- sum(g.adtm[pair,]) - pr.adtm_w - pr.adtm_k - g.adtm[pair,] # total terms not w or k
    pr.se <- sqrt(1/g.adtm[pair,] + 1/pr.adtm_w + 1/pr.adtm_k + 1/pr.adtm_kw)
    pr.zeta <- pr.delta/pr.se
    
    return(list(zeta=pr.zeta[1,], delta=pr.delta[1,],se=pr.se[1,], counts = colSums(dtm.nz), acounts = colSums(g.adtm)))
  } else {
    return(list(zeta=g.zeta,delta=g.delta,se=g.se,counts=g.counts,acounts=g.adtm))
  }
}


### fightin' words plotting

makeTransparent<-function(someColor, alpha=100)
{
  newColor<-col2rgb(someColor)
  apply(newColor, 2, function(curcoldata){rgb(red=curcoldata[1], green=curcoldata[2],
                                              blue=curcoldata[3],alpha=alpha, maxColorValue=255)})
}

fw.ggplot.groups <- function(fw.ch, groups.use = as.factor(rownames(fw.ch$zeta)), max.words = 50, max.countrank = 400, colorpalette=rep("black",length(groups.use)), sizescale=2, title="Comparison of Terms by Groups", subtitle = "", caption = "Group-specific terms are ordered by Fightin' Words statistic (Monroe, et al. 2008)") {
  if (is.null(dim(fw.ch$zeta))) {## two-group fw object consists of vectors, not matrices
    zetarankmat <- cbind(rank(-fw.ch$zeta),rank(fw.ch$zeta))
    colnames(zetarankmat) <- groups.use
    countrank <- rank(-(fw.ch$counts))
  } else {
    zetarankmat <- apply(-fw.ch$zeta[groups.use,],1,rank)
    countrank <- rank(-colSums(fw.ch$counts))
  }
  wideplotmat <- as_tibble(cbind(zetarankmat,countrank=countrank))
  wideplotmat$term=names(countrank)
  #rankplot <- gather(wideplotmat, party, zetarank, 1:ncol(zetarankmat))
  rankplot <- gather(wideplotmat, groups.use, zetarank, 1:ncol(zetarankmat))
  rankplot$plotsize <- sizescale*(50/(rankplot$zetarank))^(1/4)
  rankplot <- rankplot[rankplot$zetarank < max.words + 1 & rankplot$countrank<max.countrank+1,]
  rankplot$groups.use <- factor(rankplot$groups.use,levels=groups.use)
  
  p <- ggplot(rankplot, aes((nrow(rankplot)-countrank)^1, -(zetarank^1), colour=groups.use)) + 
    geom_point(show.legend=F,size=sizescale/2) + 
    theme_classic() +
    theme(axis.ticks=element_blank(), axis.text=element_blank() ) +
    ylim(-max.words,40) +
    facet_grid(groups.use ~ .) +
    geom_text_repel(aes(label = term), size = rankplot$plotsize, point.padding=.05, max.overlaps = 1000,
                    box.padding = unit(0.20, "lines"), show.legend=F) +
    scale_colour_manual(values = alpha(colorpalette, .7)) + 
    #labs(x="Terms used more frequently overall →", y="Terms used more frequently by group →",  title=title, subtitle=subtitle , caption=caption) 
    labs(x=paste("Terms used more frequently overall -->"), y=paste("Terms used more frequently by group -->"),  title=title, subtitle=subtitle , caption=caption) 
  
}

### fightin' words table

fw.keys <- function(fw.ch,n.keys=10) {
  n.groups <- nrow(fw.ch$zeta)
  keys <- matrix("",n.keys,n.groups)
  colnames(keys) <- rownames(fw.ch$zeta)
  
  for (g in 1:n.groups) {
    keys[,g] <- names(sort(fw.ch$zeta[g,],dec=T)[1:n.keys])
  }
  keys
}

```


## Reading and Preparing Data 

- The Poliblogs08 data set consists of 13,246 blogposts scraped from 6 blogs, along with a "rating" as "Conservative" or "Liberal."

```{r}
poliblogsFull <- read.csv("/Users/taegyoon/Dropbox/Materials/teaching/spring24_nlp/week_04_keyword/poliblogs2008.csv", colClasses = c("character", "character", "character", "factor", "integer", "factor")) # make sure that you define your own directory to the data
head(poliblogsFull)
```

- Today we'll be using a sample of 5,000 blogs from the Poliblogs08 data set, provided in the **stm** package (a package for structural topic modeling): `poliblog5k.meta`
- The `poliblog5k.meta` object only contains a small snippet---the first 50 characters---of the text in the column `poliblog5k.meta$text`. 
- We'll need full texts to estimate Fightin' Words
- The following takes full texts from the original data set

```{r}
poliblog5k.sample <- as.integer(rownames(poliblog5k.meta)) # the row indices from the original data
poliblog5k.meta$fulltext <- poliblogsFull$documents[poliblog5k.sample] # get the full text from the original data set 
```


## Preprocessing and tokenization

```{r}
poliblog5k.corpus <- corpus(poliblog5k.meta, # we first build a corpus
                            text_field = "fulltext")

tokens <- poliblog5k.corpus %>%
  tokens(remove_numbers = TRUE, 
         remove_punct = TRUE, 
         remove_symbols = TRUE) %>%
  tokens_remove(stopwords("english")) %>%
  tokens_wordstem() %>%
  tokens_tolower()
```


## Building a document-feature matrix

```{r}
poliblog5k.dfm <- dfm(tokens) # create a document-feature matrix
poliblog5k.dfm <- dfm_trim(poliblog5k.dfm, # remove words that appear in at least 50 documents
                           min_docfreq = 51, 
                           docfreq_type = "count")
dim(poliblog5k.dfm) # check dimension
```


## Estiamte Fighting' Words

```{r}
fw.blogideo <- fwgroups(poliblog5k.dfm,
                        groups = poliblog5k.dfm$rating)
```


## Get and show the top words per group by zeta

```{r echo=TRUE, results="asis"}
fwkeys.blogideo <- fw.keys(fw.blogideo, n.keys=20)
kable(fwkeys.blogideo)
```


## Plot the Fightin' Words

```{r, fig.height=10, fig.width=8}
p.fw.blogideo <- fw.ggplot.groups(fw.blogideo,
                                  sizescale = 5, # the size of the dot/text
                                  max.words = 100, # the number of words to display based on zeta
                                  max.countrank = 400, # the number of words to display based on frequencies
                                  colorpalette = c("red","blue"))
p.fw.blogideo
```